"""
Adapted from https://github.com/raghakot/keras-resnet cifar10 example
Train ResNet-50 on the facial expression small images dataset.

"""

from keras.datasets import cifar10
from keras.preprocessing.image import ImageDataGenerator
from keras.utils import np_utils
from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping, ModelCheckpoint
from setup_facial import FACIAL, FACIALModel
import numpy as np
import resnet
import os
import argparse

if __name__ == "__main__":
  parser = argparse.ArgumentParser()
  parser.add_argument("--model_weights", default=None, help="the path of the pre-trained model weights")
  parser.add_argument("--save_prefix", default="resnet50_imgsize48", help="the file name prefix for saving the model")

  args = vars(parser.parse_args())

  batch_size = 100
  nb_classes = 7
  nb_epoch = 200
  data_augmentation = True

  # input image dimensions
  img_rows, img_cols = 48, 48
  # The CIFAR10 images are RGB.
  img_channels = 1

  # The data, shuffled and split between train and test sets:
  data, model = FACIAL(), FACIALModel(args["model_weights"], use_log=False)
  model = model.model
  if os.path.exists("resnet50_imgsize48.ckpt"):
    print("Loading pre-trained model")
    model.load_weights("resnet50_imgsize48.ckpt")
  model.compile(loss='categorical_crossentropy',
                optimizer='adam',
                metrics=['accuracy'])
  checkpointer = ModelCheckpoint(filepath="resnet50_imgsize48.ckpt", verbose=1, save_best_only=True)
  if not data_augmentation:
      print('Not using data augmentation.')
      model.fit(data.train_data, data.train_labels,
                batch_size=batch_size,
                nb_epoch=nb_epoch,
                validation_data=(X_test, Y_test),
                shuffle=True,
                callbacks=[checkpointer])
  else:
      print('Using real-time data augmentation.')
      # This will do preprocessing and realtime data augmentation:
      datagen = ImageDataGenerator(
          featurewise_center=False,  # set input mean to 0 over the dataset
          samplewise_center=False,  # set each sample mean to 0
          featurewise_std_normalization=False,  # divide inputs by std of the dataset
          samplewise_std_normalization=False,  # divide each input by its std
          zca_whitening=False,  # apply ZCA whitening
          rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
          width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
          height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
          horizontal_flip=True,  # randomly flip images
          vertical_flip=False)  # randomly flip images

      # Compute quantities required for featurewise normalization
      # (std, mean, and principal components if ZCA whitening is applied).
      datagen.fit(data.train_data)

      # Fit the model on the batches generated by datagen.flow().
      model.fit_generator(datagen.flow(data.train_data, data.train_labels, batch_size=batch_size),
                          steps_per_epoch=data.train_data.shape[0] // batch_size,
                          validation_data=(data.test_data, data.test_labels),
                          epochs=nb_epoch, verbose=1, max_q_size=100,
                          callbacks=[checkpointer])
  model_json = model.to_json()
  json_file_name = "{}_model.json".format(args["save_prefix"])
  with open(json_file_name, "w") as json_file:
    json_file.write(model_json)
  print("Save model spec to {}".format(json_file_name))

  weight_file_name = "{}_weights.h5".format(args["save_prefix"])
  model.save_weights(weight_file_name)
  print("Save model weights to {}".format(weight_file_name))
